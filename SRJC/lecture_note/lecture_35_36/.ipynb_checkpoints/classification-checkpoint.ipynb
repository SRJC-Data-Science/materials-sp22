{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4671976f",
   "metadata": {},
   "source": [
    "# Lecture Notes - Classification #\n",
    "\n",
    "**Helpful Resource:**\n",
    "- [Python Reference](http://data8.org/sp22/python-reference.html): Cheat sheet of helpful array & table methods used in Data 8!\n",
    "\n",
    "**Recommended Readings:**\n",
    "- [Classification](https://inferentialthinking.com/chapters/17/Classification.html)\n",
    "- [Nearest Neigbors](https://inferentialthinking.com/chapters/17/1/Nearest_Neighbors.html)\n",
    "- [Training & Testing](https://inferentialthinking.com/chapters/17/2/Training_and_Testing.html)\n",
    "- [Rows of Tables](https://inferentialthinking.com/chapters/17/3/Rows_of_Tables.html)\n",
    "- [Implementing the Classifier](https://inferentialthinking.com/chapters/17/4/Implementing_the_Classifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "import numpy as np\n",
    "import math\n",
    "import datascience\n",
    "from datascience import *\n",
    "\n",
    "# These lines set up the plotting functionality and formatting.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Video, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746fa7",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Classification in data science is about learning how to make predictions based on the past examples.\n",
    "\n",
    "Classificaiton is a technique often used in machine learning.\n",
    "\n",
    "### Sample Predictions\n",
    "\n",
    "1. Credit card companies: Is this transaction fraudulent?\n",
    "2. Doctors: Does this patient have cancer?\n",
    "3. Election: Are you going to vote this candidate or another?\n",
    "\n",
    "### Data and Training Set\n",
    "\n",
    "To make predictions, it requires data. Classification involves looking for patterns, and to find patterns, we need data.\n",
    "\n",
    "Essentially, we need **training data**: a bunch of observations. Observation is an entity that can be an individual or situation where we know the class of each observation, such as whether the transaction is fraudulent, the patient has cancer, etc. The typical classes are yes or no. We usually use 1 (class 1) to represent yes and 0 (class 0) represent no.\n",
    "\n",
    "**Class** is a result of an observation.\n",
    "\n",
    "**Training set** is a collection of these pre-classified observations.\n",
    "\n",
    "Then we will need a **classification algorithm** to analyze the training set, and then come up with a **classifier** which is an algorithm for predicting the class of future observations. The purpose of a classifier is to classify unseen data that is similar to the training data. \n",
    "\n",
    "**Test dataset** help us determine the accuracy of our predictions by comparing the actual classes of the observations with the classes that our classifier predicts.\n",
    "\n",
    "Training is a process of trial and error.\n",
    "\n",
    "Note that classifiers do not need to be perfect to be useful. They can be useful even if their accuracy is less than 100%. For instance, if the online dating site occasionally makes a bad recommendation, that’s OK; their customers already expect to have to meet many people before they’ll find someone they hit it off with. Of course, you don’t want the classifier to make too many errors — but it doesn’t have to get the right answer every single time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454d785",
   "metadata": {},
   "source": [
    "### Computer Vision: How we're teaching computers to understand pictures\n",
    "\n",
    "Fei-Fei Li is a computer science professor at Stanford University and founding director of the Stanford Institute for Human-Centered AI, an organization that aims to advance AI research, education, policy and practice to improve the human condition.\n",
    "\n",
    "When a very young child looks at a picture, she can identify simple elements: \"cat,\" \"book,\" \"chair.\" Now, computers are getting smart enough to do that too. What's next? In a thrilling talk, computer vision expert Fei-Fei Li describes the state of the art -- including the database of 15 million photos her team built to \"teach\" a computer to understand pictures -- and the key insights yet to come.\n",
    "\n",
    "Prof. Li also spoke at The Power of Artificial Intelligence - US Congressional Hearing, in June 26th, 2018, along with Jaime Carbonell, director, Language Technologies Institute; Allen Newell Professor, School of Computer Science, Carnegie Mellon University; Tim Persons, chief scientist, GAO; Greg Brockman, co-founder and chief technology officer, OpenAI\n",
    "\n",
    "https://ai-4-all.org/fei-fei-li-speaks-about-ai-on-capitol-hill/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbafe2a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4238195152.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?utm_campaign=tedspread&utm_medium=referral&utm_source=tedcomshare\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "video_path = \"2015-fei-fei-li-007-1200k.mp4\"\n",
    "\n",
    "display(Video(video_path, width=600, embed=True))\n",
    "\n",
    "https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?utm_campaign=tedspread&utm_medium=referral&utm_source=tedcomshare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1fc55",
   "metadata": {},
   "source": [
    "### Classification Example \n",
    "\n",
    "#### Classifying Patients\n",
    "\n",
    "Class 1 are patients with CKD (Chronic Kidney Disease)\n",
    "\n",
    "Class 0 are patients without CKD\n",
    "\n",
    "Each row represents a blood test result of a patient. Each column represents an element tested in the blood. Some are numerical, others are strings. We use numeric data for developing classifier algorithms. We also call each column as a **feature** when developing classifier algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080352a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 1: load the data to a table and visually inspect the dataset\n",
    "\n",
    "ckd = Table.read_table('ckd.csv').relabeled('Blood Glucose Random', 'Glucose')\n",
    "ckd.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccae56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 2: check numbers of classes in the dataset, \n",
    "#         use tbl.group() function\n",
    "\n",
    "# 2 classes in the dataset, \n",
    "#         0 represents patients without CKD\n",
    "#         1 represents patients with CKD\n",
    "\n",
    "ckd.group('Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006cec10",
   "metadata": {},
   "source": [
    "## Visualize the relation between the two variables ##\n",
    "\n",
    "Class 1 Blue dots are patients with CKD; \n",
    "\n",
    "Class 0 Gold dots are patients without CKD.\n",
    "\n",
    "Each element in the blood contributes a bit to having or not having the CKD. Typtically more data is better to develop an algorithm to train a machine to make decision, concurrently could be more complex. Let's start with visualizing the relation between 2 elements (variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7723690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 3: create a color table for plotting a graph,\n",
    "#         make sure to have a common column as in the dataset,\n",
    "#         the common column is the results of all observations in rows,\n",
    "#         we typtically call it Class\n",
    "\n",
    "color_table = Table().with_columns(\n",
    "    'Class', make_array(1, 0),\n",
    "    'Color', make_array('darkblue', 'gold')\n",
    ")\n",
    "\n",
    "# step 4: use tbl.join() function to combine the color table with the dataset\n",
    "#         join ckd as the primary table with color_table\n",
    "ckd_color = ckd.join('Class', color_table)\n",
    "ckd_color.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86beea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 2\n",
    "\n",
    "# step 4: use tbl.join() function to combine the color table with the dataset\n",
    "#         join color_table as the primary table with ckd\n",
    "ckd_color_alt = ckd.join('Class', color_table)\n",
    "ckd_color_alt.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2160c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 5a: draw a scatter plot to visualize the relation between Hemoglobin and Glucose\n",
    "ckd_color.scatter('Hemoglobin', 'Glucose', group='Color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 5b: draw a scatter plot to visualize the relation between WBC count and Glucose\n",
    "ckd_color.scatter('White Blood Cell Count', 'Glucose', group='Color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ec76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 5c: draw a scatter plot to visualize the relation between WBC count and Hemoglobin\n",
    "ckd_color.scatter('White Blood Cell Count', 'Hemoglobin', group='Color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560adf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at a 3D view when we combine Glucose, Hemoglobin, and WBC count\n",
    "\n",
    "ax = plots.figure(figsize=(8,10)).add_subplot(111, projection='3d')\n",
    "ax.scatter(ckd_color.column('Glucose'), \n",
    "           ckd_color.column('Hemoglobin'), \n",
    "           ckd_color.column('White Blood Cell Count'), \n",
    "           c=ckd_color.column('Color'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050963a5",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "Theoretically, we can build a classifier algorithm with many features that will make highly accurate predictions.\n",
    "\n",
    "However:\n",
    "1. Any plots of 4D or above could be challenging to visualize in human eyes.\n",
    "3. Computational resources may limit numbers of features for classifier algorithm development.\n",
    "\n",
    "#### Side note: \n",
    "Matplotlib doesn't have direct support for 4D plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578b5f7",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Classifier ##\n",
    "\n",
    "There are many pre-classified samples on the graph. The unseen sample could be located at the cluster where has mixed of pre-classified samples of having and not having the disease.\n",
    "\n",
    "We will need a way to determine the prediction, and we use Nearest Neighbors Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee679e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.2.1. Measuring in Standard Units -\n",
    "#         https://inferentialthinking.com/chapters/15/2/Regression_Line.html#measuring-in-standard-units\n",
    "def standard_units(x):\n",
    "    \"Convert the input x to standard units.\"\n",
    "    return (x - np.mean(x))/np.std(x)\n",
    "\n",
    "# function distance() which returned the distance between two points. \n",
    "# We used it in two-dimensions, but the great news is that \n",
    "# the function doesn’t care how many dimensions there are! \n",
    "# It just subtracts the two arrays of coordinates (no matter how long the arrays are), \n",
    "# squares the differences and adds up, and then takes the square root. \n",
    "# To work in multiple dimensions, we don’t have to change the code at all.\n",
    "\n",
    "def distance(point1, point2):\n",
    "    \"\"\"The distance between two arrays of numbers.\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def all_distances(training, point):\n",
    "    \"\"\"The distance between p (an array of numbers) and the numbers in row i of attribute_table.\"\"\"\n",
    "    attributes = training.drop('Class')\n",
    "    def distance_from_point(row):\n",
    "        return distance(point, np.array(row))\n",
    "    return attributes.apply(distance_from_point)\n",
    "\n",
    "def table_with_distances(training, point):\n",
    "    \"\"\"A copy of the training table with the distance from each row to array p.\"\"\"\n",
    "    return training.with_column('Distance', all_distances(training, point))\n",
    "\n",
    "def closest(training, point, k):\n",
    "    \"\"\"A table containing the k closest rows in the training table to array p.\"\"\"\n",
    "    with_dists = table_with_distances(training, point)\n",
    "    sorted_by_distance = with_dists.sort('Distance')\n",
    "    topk = sorted_by_distance.take(np.arange(k))\n",
    "    return topk\n",
    "\n",
    "def show_closest_2d(point, data_tbl, col1, col2, k):\n",
    "    \"\"\"point = array([x,y]) \n",
    "    gives the coordinates of a new point\n",
    "    shown in red\"\"\"\n",
    "    \n",
    "    train_features = data_tbl.select('Class', col1, col2)\n",
    "    t = closest(train_features, point, k)\n",
    "    x_closest = t.row(0).item(1)\n",
    "    y_closest = t.row(0).item(2)\n",
    "    data_tbl.scatter(col1, col2, group='Color')\n",
    "    plots.scatter(point.item(0), point.item(1), color='red', s=30)\n",
    "    plots.plot(make_array(point.item(0), x_closest), make_array(point.item(1), y_closest), lw=2);\n",
    "    \n",
    "    \n",
    "def show_closest_3d(point, data_tbl, col1, col2, col3, k):\n",
    "    \"\"\"point = array([x,y]) \n",
    "    gives the coordinates of a new point\n",
    "    shown in red\"\"\"\n",
    "    \n",
    "    trainFeatures = data_tbl.select('Class', col1, col2, col3)\n",
    "    t = closest(trainFeatures, point, k)\n",
    "    x_closest = t.row(0).item(1)\n",
    "    y_closest = t.row(0).item(2)\n",
    "    z_closest = t.row(0).item(3)\n",
    "    ax = plots.figure(figsize=(8,10)).add_subplot(111, projection='3d')\n",
    "    ax.scatter(data_tbl.column(col1), \n",
    "               data_tbl.column(col2), \n",
    "               data_tbl.column(col3), \n",
    "               c=data_tbl.column('Color'));\n",
    "    ax.scatter(point.item(0), point.item(1), point.item(2), color='red', s=30)\n",
    "    ax.plot(make_array(point.item(0), x_closest), make_array(point.item(1), y_closest), make_array(point.item(2), z_closest), lw=2);\n",
    "    \n",
    "def majority(label, table):\n",
    "    \"\"\"The majority element in a column of a table.\n",
    "    \n",
    "    This function takes two arguments:\n",
    "      label: The label of a column, a string.\n",
    "      table: A table.\n",
    "     \n",
    "    It returns the most common value in the label column of the table.\n",
    "    In case of a tie, it returns any one of the most common values.    \n",
    "    \"\"\"\n",
    "    return table.group(label).sort('count', descending=True).column(label).item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906521b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888cfc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 6: convert features into standard units\n",
    "ckd_std_units = Table().with_columns(\n",
    "    'Hemoglobin', standard_units(ckd.column('Hemoglobin')),\n",
    "    'Glucose', standard_units(ckd.column('Glucose')),\n",
    "    'White Blood Cell Count', standard_units(ckd.column('White Blood Cell Count')),\n",
    "    'Class', ckd.column('Class')\n",
    ")\n",
    "ckd_std_units.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d9d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "color_table = Table().with_columns(\n",
    "    'Class', make_array(1, 0),\n",
    "    'Color', make_array('darkblue', 'gold')\n",
    ")\n",
    "\n",
    "# option 1\n",
    "#ckd_std_units_color = ckd_std_units.join('Class', color_table)\n",
    "#or\n",
    "# option 2\n",
    "ckd_std_units_color = color_table.join('Class', ckd_std_units)\n",
    "#ckd_std_units_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abd1a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# new patient: Hemoglobin (0), Glucose (1.5)\n",
    "ckd_std_units_color.scatter('Hemoglobin', 'Glucose', group='Color')\n",
    "plots.scatter(0, 1.5, color='red', s=30) # s=30 is the size of the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208defc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this example, Alice's Hemoglobin is 0 and her Glucose is 1.5.\n",
    "#   we look for the patient closest to Alice\n",
    "alice = make_array(0, 1.5)\n",
    "show_closest_2d(alice, ckd_std_units_color, 'Hemoglobin', 'Glucose', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, Alice's WBC count is 0.6 and her Glucose is 1.5.\n",
    "#   we look for the patient closest to Alice\n",
    "alice = make_array(0.6, 1.5)\n",
    "show_closest_2d(alice, ckd_std_units_color, 'White Blood Cell Count', 'Glucose', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1813137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this example, Alice's  is WBC count 0.6 and her Hemoglobin is 1.5.\n",
    "#   we look for the patient closest to Alice\n",
    "alice = make_array(0.6, 1.5)\n",
    "show_closest_2d(alice, ckd_std_units_color, 'White Blood Cell Count', 'Hemoglobin', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a9529",
   "metadata": {},
   "source": [
    "## Use more features for more accurate predictions ##\n",
    "As you see, our predictions are not consistent when we look at 2 features in the blood tests.\n",
    "\n",
    "Let see what happens if we check out all 3 features - Glucose, Hemoglobin and WBC count. \n",
    "\n",
    "In general, more features are put into consideration, more accurate predictions will be.\n",
    "\n",
    "We will have a 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9672f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x - gloucose: 0, y - hemoglobin: 1.5, z - white blood cell count: 0.6\n",
    "alice = make_array(0, 1.5, 0.6)\n",
    "show_closest_3d(alice, ckd_std_units_color, 'Glucose', 'Hemoglobin', 'White Blood Cell Count', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2d3e9",
   "metadata": {},
   "source": [
    "## One neighbor vs. multiple neighbors\n",
    "\n",
    "So far, we make our prediction based on only 1 nearest neighbor. Sometimes, the second nearest neighbor is only tiny bit away but may have an opposite result as the first nearest neighbor, and that make our prediction uncertain.\n",
    "\n",
    "So, we are going to take vote to a few more nearest neighbors, then we consider the majority and make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314cd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AllFeatures = ckd_std_units.drop('Color')\n",
    "\n",
    "# gloucose: 0, hemoglobin: 1.5, white blood cell count: 0.6\n",
    "newPatient = make_array(0, 1.5, 0.6)\n",
    "topk = closest(AllFeatures, newPatient, 7)\n",
    "topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority(\"Class\", topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140667e0",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors ##\n",
    "\n",
    "[K-Nearest Neighbors (k-NN)](https://inferentialthinking.com/chapters/17/1/Nearest_Neighbors.html) is a classification algorithm.  Given some numerical *attributes* (also called *features*) of an unseen example, it decides which category that example belongs to based on its similarity to previously seen examples.\n",
    "\n",
    "Predicting the category of an example is called *labeling*, and the predicted category is also called a *label*.\n",
    "\n",
    "The whole idea of K-Nearest Neighbors is to compute the distances between an unseen example and the previously seen examples. That will produce a bunch of distance values. Out of the distance values, we examine a few of the nearest neighbors and predict the class for the unseen example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0a3d1",
   "metadata": {},
   "source": [
    "## Implement a Movie Genre Classifier (Project 3) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c735c",
   "metadata": {},
   "source": [
    "An attribute (feature) we have about each movie is *the proportion of times a particular word appears in the movie*, and the labels are two movie genres: comedy and thriller.  The algorithm requires many previously seen examples for which both the attributes and labels are known: that's the `train_movies` table.\n",
    "\n",
    "We'll be trying to predict each movie's genre from the text of its screenplay. In particular, we have compiled a list of 5,000 words that occur in conversations between movie characters. For each movie, our dataset tells us the frequency with which each of these words occurs in certain conversations in its screenplay. All words have been converted to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f86eba",
   "metadata": {},
   "source": [
    "## Steps to implement classifier ##\n",
    "\n",
    "1. load the data to a table and visually inspect the dataset\n",
    "2. check numbers of classes in the dataset, use tbl.group() function\n",
    "3. split the dataset into a training set and a test set\n",
    "4. pick a sample from the test set and pick 2 features(columns) to test the classifier algorithm\n",
    "5. compute the distances between an unseen sample and the previously seen samples\n",
    "6. examine a few of the nearest neighbors\n",
    "7. predict the class for the unseen sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8612c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: load the table\n",
    "\n",
    "movies = Table.read_table('movies.csv')\n",
    "#movies.take(0).select(0, 1, 2, 3, 4, 'hei', 'you', 1042, 'fun')\n",
    "movies.take(100).select(0, 1, 2, 3, 4, 14, 49, 1042, 4004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec116913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# step 2: check numbers of classes in the dataset, use tbl.group() function\n",
    "movies.group('Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: split the dataset into a training set and a test set\n",
    "\n",
    "training_proportion = 17/20\n",
    "\n",
    "num_movies = movies.num_rows\n",
    "num_train = int(num_movies * training_proportion)\n",
    "num_test = num_movies - num_train\n",
    "\n",
    "train_movies = movies.take(np.arange(num_train))\n",
    "test_movies = movies.take(np.arange(num_train, num_movies))\n",
    "\n",
    "print(\"Training: \",   train_movies.num_rows, \";\",\n",
    "      \"Test: \",       test_movies.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694dba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 4: pick a sample from the test set and \n",
    "#         pick 2 features(columns) to test the classifier algorithm\n",
    "test_movies.where('Title', 'monty python and the holy grail')\n",
    "\n",
    "# we use 'water' and 'feel' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: compute the distances between an unseen sample and \n",
    "#         the previously seen samples\n",
    "\n",
    "# Given functions:\n",
    "\n",
    "title_index = movies.index_by('Title')\n",
    "def row_for_title(title):\n",
    "    \"\"\"Return the row for a title, similar to the following expression (but faster)\n",
    "    \n",
    "    movies.where('Title', title).row(0)\n",
    "    \"\"\"\n",
    "    return title_index.get(title)[0]\n",
    "\n",
    "\n",
    "def plot_with_two_features(test_movie, training_movies, x_feature, y_feature):\n",
    "    \"\"\"Plot a test movie and training movies using two features.\"\"\"\n",
    "    test_row = row_for_title(test_movie)\n",
    "    distances = Table().with_columns(\n",
    "            x_feature, [test_row.item(x_feature)],\n",
    "            y_feature, [test_row.item(y_feature)],\n",
    "            'Color',   ['unknown'],\n",
    "            'Title',   [test_movie]\n",
    "        )\n",
    "    for movie in training_movies:\n",
    "        row = row_for_title(movie)\n",
    "        distances.append([row.item(x_feature), row.item(y_feature), row.item('Genre'), movie])\n",
    "    distances.scatter(x_feature, y_feature, group='Color', labels='Title', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace369e8",
   "metadata": {},
   "source": [
    "## Compute distance ##\n",
    "\n",
    "We refer to a straight-line distance.\n",
    "\n",
    "**This distance is called the Euclidean (\"yoo-KLID-ee-un\") distance, whose formula is $\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$.**\n",
    "\n",
    "For example, in the movie *Clerks.* (in the training set), 0.00016293 of all the words in the movie are \"water\" and 0.00154786 are \"feel\".  Its distance from *Monty Python and the Holy Grail* on this 2-word feature set is $\\sqrt{(0.000804074 - 0.000162933)^2 + (0.0010721 - 0.00154786)^2} \\approx 0.000798379$.  (If we included more or different features, the distance could be different.)\n",
    "\n",
    "*Monty Python and the Holy Grail* has 0.000804074 on \"water\" and 0.0010721 on \"feel\".\n",
    "\n",
    "If we consider more than 2 words for predictions, we make multi-dimensional space. Then we will keep adding the additional dimensional spaces inside the square root.\n",
    "\n",
    "$\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2}$\n",
    "\n",
    "$\\sqrt{(a_1 - a_2)^2 + (b_1 - b_2)^2 + (c_1 - c_2)^2 + (d_1 - d_2)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test sample movie is Monty Python and the Holy Grail\n",
    "#     the features \"water\" and \"feel\" are used for prediction\n",
    "\n",
    "\n",
    "# We create a function to compute distance between between two movies - title0 and title1 \n",
    "#     based on two features -  x_feature and y_feature\n",
    "def distance_two_features(title0, title1, x_feature, y_feature):\n",
    "    \"\"\"Compute the distance between two movies with titles title0 and title1.\n",
    "    Only the features named x_feature and y_feature are used when computing the distance.\n",
    "    \"\"\"\n",
    "    row0 = row_for_title(title0)\n",
    "    row1 = row_for_title(title1)\n",
    "    return ((row0.item(x_feature) - row1.item(x_feature))**2 + (row0.item(y_feature) - row1.item(y_feature))**2)**0.5\n",
    "    #return np.sqrt((row0.item(x_feature) - row1.item(x_feature))**2 + (row0.item(y_feature) - row1.item(y_feature))**2)\n",
    "\n",
    "# Then, we create a function to compute distance between the given movie and \n",
    "#     \"monty python and the holy grail\", based on the features \"water\" and \"feel\"\n",
    "\n",
    "def distance_from_python(title):\n",
    "    \"\"\"The distance between the given movie and \"monty python and the holy grail\", \n",
    "    based on the features \"water\" and \"feel\".\n",
    "    \n",
    "    This function takes a single argument:\n",
    "      title: A string, the name of a movie.\n",
    "    \"\"\"\n",
    "    \n",
    "    return distance_two_features(title, \"monty python and the holy grail\", \"water\", \"feel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8310a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions distance_two_features() and distance_from_python()\n",
    "\n",
    "# Calculate the distance between a seen sample movie and \"Monty Python and the Holy Grail\"\n",
    "#train_movies.select('Title').show()\n",
    "distance_from_python('the silence of the lambs')\n",
    "distance_from_python('jurassic park iii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6efb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 6: examine a few of the nearest neighbors - K-Nearest Neighbors\n",
    "\n",
    "# create a table from the train_movies table.\n",
    "#      The table contains the columns of Title, Genre and \n",
    "#      only the feature we are checking - water and feel\n",
    "water_feel = train_movies.select(\"Title\", \"Genre\", \"water\", \"feel\")\n",
    "\n",
    "# use tbl.apply() function to compute the distances between \n",
    "#     \"Monty Python and the Holy Grail\" and movies in the training set\n",
    "water_feel_distance = water_feel.apply(distance_from_python, 'Title')\n",
    "\n",
    "# add the distance column to the table\n",
    "close_movies = water_feel.with_column('distance from python', water_feel_distance)\n",
    "\n",
    "# sort the table in ascending order and take the first 5 rows\n",
    "close_movies.sort('distance from python').take(np.arange(5))\n",
    "\n",
    "# alternatively, we can do it in 1 line\n",
    "#close_movies = water_feel.with_column('distance from python', water_feel.apply(distance_from_python, 'Title'))\\\n",
    "#                        .sort('distance from python')\\\n",
    "#                       .take(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7: predict the class (genre in this case) for the unseen sample\n",
    "\n",
    "# group the table by class and sort by count column in descending order\n",
    "def majority(label, table):\n",
    "    \"\"\"The majority element in a column of a table.\n",
    "    \n",
    "    This function takes two arguments:\n",
    "      label: The label of a column, a string.\n",
    "      table: A table.\n",
    "     \n",
    "    It returns the most common value in the label column of the table.\n",
    "    In case of a tie, it returns any one of the most common values.    \n",
    "    \"\"\"\n",
    "    return table.group(label).sort('count', descending=True).column(label).item(0)\n",
    "\n",
    "majority('Genre', close_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525e30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
